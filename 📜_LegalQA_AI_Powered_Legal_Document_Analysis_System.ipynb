{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# COMPLETE SETUP SCRIPT FOR LEGAL QA SYSTEM\n",
        "# Run this in Google Colab or your Python environment\n",
        "# ============================================================================\n",
        "\n",
        "# STEP 1: Install all required packages\n",
        "print(\"ğŸ”§ Installing required packages...\")\n",
        "\n",
        "# Install command for Google Colab\n",
        "!pip install streamlit pyngrok pandas scikit-learn joblib opencv-python pytesseract python-docx pdfplumber pillow openpyxl xlrd\n",
        "\n",
        "# Install tesseract for OCR (Colab only)\n",
        "!apt install -y tesseract-ocr\n",
        "\n",
        "print(\"âœ… All packages installed successfully!\")\n",
        "\n",
        "# STEP 2: Import and test all modules\n",
        "print(\"\\nğŸ§ª Testing imports...\")\n",
        "\n",
        "try:\n",
        "    import streamlit as st\n",
        "    import pandas as pd\n",
        "    import pickle\n",
        "    import os\n",
        "    import tempfile\n",
        "    import pdfplumber\n",
        "    import io\n",
        "    import logging\n",
        "    from typing import Optional, Tuple, Any, Dict, List\n",
        "    import re\n",
        "    import json\n",
        "    from datetime import datetime\n",
        "    import numpy as np\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    from PIL import Image\n",
        "    print(\"âœ… All imports successful!\")\n",
        "except ImportError as e:\n",
        "    print(f\"âŒ Import error: {e}\")\n",
        "\n",
        "# STEP 3: Create the complete legal QA app\n",
        "print(\"\\nğŸ“ Creating legal_qa_app.py...\")\n",
        "\n",
        "app_code = '''\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import os\n",
        "import tempfile\n",
        "import pdfplumber\n",
        "import io\n",
        "import logging\n",
        "from typing import Optional, Tuple, Any, Dict, List\n",
        "import re\n",
        "import json\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Page config\n",
        "st.set_page_config(\n",
        "    page_title=\"âš–ï¸ CUAD Legal Document QA System\",\n",
        "    page_icon=\"âš–ï¸\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "class LegalDocumentProcessor:\n",
        "    \"\"\"Main processor for legal documents with comprehensive error handling\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.supported_formats = ['pdf', 'txt', 'json', 'xlsx', 'csv']\n",
        "        self.model = None\n",
        "        self.vectorizer = None\n",
        "        self.setup_model()\n",
        "\n",
        "    def setup_model(self):\n",
        "        \"\"\"Initialize or create a simple risk assessment model\"\"\"\n",
        "        try:\n",
        "            # Create a simple model on the fly\n",
        "            self.create_simple_model()\n",
        "            st.success(\"âœ… Risk assessment model initialized\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Model setup error: {e}\")\n",
        "            st.error(f\"âŒ Model setup failed: {e}\")\n",
        "\n",
        "    def create_simple_model(self):\n",
        "        \"\"\"Create a simple risk assessment model\"\"\"\n",
        "        # Sample training data for legal risk assessment\n",
        "        training_data = [\n",
        "            (\"The agreement may be terminated at any time without notice\", \"High\"),\n",
        "            (\"Party shall indemnify and hold harmless against all claims\", \"High\"),\n",
        "            (\"Limitation of liability shall not exceed the contract value\", \"Medium\"),\n",
        "            (\"This agreement shall be governed by the laws of\", \"Low\"),\n",
        "            (\"Confidential information must be protected at all times\", \"Medium\"),\n",
        "            (\"Either party may terminate with 30 days written notice\", \"Medium\"),\n",
        "            (\"The effective date of this agreement is\", \"Low\"),\n",
        "            (\"Penalty for breach shall include liquidated damages\", \"High\"),\n",
        "            (\"No warranty is provided for the services rendered\", \"High\"),\n",
        "            (\"Standard business hours are defined as 9 AM to 5 PM\", \"Low\"),\n",
        "            (\"Exclusive jurisdiction lies with the courts of\", \"Medium\"),\n",
        "            (\"Force majeure events include natural disasters\", \"Low\"),\n",
        "            (\"Intellectual property rights remain with the creator\", \"Medium\"),\n",
        "            (\"Unlimited liability for gross negligence or willful misconduct\", \"High\"),\n",
        "            (\"Contract automatically renews unless terminated\", \"Medium\"),\n",
        "            (\"Termination without cause\", \"High\"),\n",
        "            (\"License agreement expires\", \"Medium\"),\n",
        "            (\"Data processing agreement\", \"Medium\"),\n",
        "            (\"Warranty disclaimer\", \"High\"),\n",
        "            (\"Governing law provisions\", \"Low\")\n",
        "        ]\n",
        "\n",
        "        texts, labels = zip(*training_data)\n",
        "\n",
        "        # Create and train model\n",
        "        self.vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
        "        X = self.vectorizer.fit_transform(texts)\n",
        "\n",
        "        self.model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "        self.model.fit(X, labels)\n",
        "\n",
        "    def process_file(self, uploaded_file) -> Dict[str, Any]:\n",
        "        \"\"\"Process uploaded file and return structured results\"\"\"\n",
        "        try:\n",
        "            filename = uploaded_file.name\n",
        "            file_extension = filename.split('.')[-1].lower()\n",
        "\n",
        "            if file_extension not in self.supported_formats:\n",
        "                return {\n",
        "                    'success': False,\n",
        "                    'error': f\"Unsupported file format: {file_extension}\",\n",
        "                    'supported_formats': self.supported_formats\n",
        "                }\n",
        "\n",
        "            # Extract content based on file type\n",
        "            content = self.extract_content(uploaded_file, file_extension)\n",
        "\n",
        "            if not content:\n",
        "                return {\n",
        "                    'success': False,\n",
        "                    'error': \"Could not extract content from file\"\n",
        "                }\n",
        "\n",
        "            # Perform analysis\n",
        "            analysis_results = self.analyze_content(content, filename)\n",
        "\n",
        "            return {\n",
        "                'success': True,\n",
        "                'filename': filename,\n",
        "                'file_type': file_extension,\n",
        "                'content_preview': content[:500] + \"...\" if len(content) > 500 else content,\n",
        "                'analysis': analysis_results,\n",
        "                'processed_at': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"File processing error: {e}\")\n",
        "            return {\n",
        "                'success': False,\n",
        "                'error': f\"Processing failed: {str(e)}\"\n",
        "            }\n",
        "\n",
        "    def extract_content(self, uploaded_file, file_extension: str) -> str:\n",
        "        \"\"\"Extract text content from various file formats\"\"\"\n",
        "        try:\n",
        "            if file_extension == 'pdf':\n",
        "                return self.extract_pdf_content(uploaded_file)\n",
        "            elif file_extension == 'txt':\n",
        "                return self.extract_txt_content(uploaded_file)\n",
        "            elif file_extension == 'json':\n",
        "                return self.extract_json_content(uploaded_file)\n",
        "            elif file_extension in ['xlsx', 'csv']:\n",
        "                return self.extract_spreadsheet_content(uploaded_file, file_extension)\n",
        "            else:\n",
        "                return \"\"\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Content extraction error: {e}\")\n",
        "            raise Exception(f\"Content extraction failed: {e}\")\n",
        "\n",
        "    def extract_pdf_content(self, uploaded_file) -> str:\n",
        "        \"\"\"Extract text from PDF\"\"\"\n",
        "        try:\n",
        "            with pdfplumber.open(io.BytesIO(uploaded_file.read())) as pdf:\n",
        "                text_parts = []\n",
        "                for page in pdf.pages:\n",
        "                    page_text = page.extract_text()\n",
        "                    if page_text:\n",
        "                        text_parts.append(page_text.strip())\n",
        "                return \"\\\\n\\\\n\".join(text_parts)\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"PDF extraction failed: {e}\")\n",
        "\n",
        "    def extract_txt_content(self, uploaded_file) -> str:\n",
        "        \"\"\"Extract text from TXT file\"\"\"\n",
        "        try:\n",
        "            return uploaded_file.read().decode('utf-8')\n",
        "        except UnicodeDecodeError:\n",
        "            try:\n",
        "                uploaded_file.seek(0)\n",
        "                return uploaded_file.read().decode('latin-1')\n",
        "            except Exception as e:\n",
        "                raise Exception(f\"Text extraction failed: {e}\")\n",
        "\n",
        "    def extract_json_content(self, uploaded_file) -> str:\n",
        "        \"\"\"Extract content from JSON file\"\"\"\n",
        "        try:\n",
        "            json_data = json.load(uploaded_file)\n",
        "            # Convert JSON to readable text\n",
        "            return json.dumps(json_data, indent=2)\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"JSON extraction failed: {e}\")\n",
        "\n",
        "    def extract_spreadsheet_content(self, uploaded_file, file_extension: str) -> str:\n",
        "        \"\"\"Extract content from spreadsheet files\"\"\"\n",
        "        try:\n",
        "            if file_extension == 'xlsx':\n",
        "                df = pd.read_excel(uploaded_file)\n",
        "            else:  # csv\n",
        "                df = pd.read_csv(uploaded_file)\n",
        "\n",
        "            # Convert dataframe to text representation\n",
        "            text_parts = []\n",
        "            text_parts.append(f\"Spreadsheet Analysis - {len(df)} rows and {len(df.columns)} columns\")\n",
        "            text_parts.append(\"\\\\nColumn Headers: \" + \", \".join(df.columns.tolist()))\n",
        "\n",
        "            # Add sample data for analysis\n",
        "            if len(df) > 0:\n",
        "                text_parts.append(\"\\\\nSample Data:\")\n",
        "                text_parts.append(df.head(3).to_string())\n",
        "\n",
        "                # Create text representation for legal analysis\n",
        "                for col in df.columns:\n",
        "                    if df[col].dtype == 'object':  # Text columns\n",
        "                        sample_values = df[col].dropna().head(10).tolist()\n",
        "                        text_parts.append(f\"\\\\n{col}: \" + \" | \".join(str(v) for v in sample_values))\n",
        "\n",
        "            return \"\\\\n\".join(text_parts)\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"Spreadsheet extraction failed: {e}\")\n",
        "\n",
        "    def analyze_content(self, content: str, filename: str) -> Dict[str, Any]:\n",
        "        \"\"\"Perform comprehensive analysis on extracted content\"\"\"\n",
        "        try:\n",
        "            # Basic content analysis\n",
        "            word_count = len(content.split())\n",
        "            char_count = len(content)\n",
        "\n",
        "            # Extract potential clauses (sentences)\n",
        "            sentences = re.split(r'[.!?]+', content)\n",
        "            clauses = [s.strip() for s in sentences if len(s.strip()) > 20]\n",
        "\n",
        "            # Risk analysis using ML model\n",
        "            risk_analysis = self.perform_risk_analysis(clauses) if self.model and self.vectorizer else None\n",
        "\n",
        "            # Key term extraction\n",
        "            key_terms = self.extract_key_terms(content)\n",
        "\n",
        "            # Document structure analysis\n",
        "            structure_analysis = self.analyze_structure(content)\n",
        "\n",
        "            return {\n",
        "                'basic_stats': {\n",
        "                    'word_count': word_count,\n",
        "                    'character_count': char_count,\n",
        "                    'clause_count': len(clauses)\n",
        "                },\n",
        "                'risk_analysis': risk_analysis,\n",
        "                'key_terms': key_terms,\n",
        "                'structure': structure_analysis,\n",
        "                'clauses_sample': clauses[:5]  # First 5 clauses for preview\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Content analysis error: {e}\")\n",
        "            return {'error': f\"Analysis failed: {str(e)}\"}\n",
        "\n",
        "    def perform_risk_analysis(self, clauses: List[str]) -> Dict[str, Any]:\n",
        "        \"\"\"Perform ML-based risk analysis on clauses\"\"\"\n",
        "        try:\n",
        "            if not clauses or len(clauses) == 0:\n",
        "                return {'error': 'No clauses found for analysis'}\n",
        "\n",
        "            # Filter out very short clauses\n",
        "            valid_clauses = [c for c in clauses if len(c.strip()) > 10]\n",
        "            if not valid_clauses:\n",
        "                return {'error': 'No valid clauses found for analysis'}\n",
        "\n",
        "            # Vectorize clauses\n",
        "            X = self.vectorizer.transform(valid_clauses)\n",
        "\n",
        "            # Predict risk levels\n",
        "            risk_predictions = self.model.predict(X)\n",
        "            risk_probabilities = self.model.predict_proba(X)\n",
        "\n",
        "            # Analyze results\n",
        "            risk_summary = {\n",
        "                'High': sum(1 for r in risk_predictions if r == 'High'),\n",
        "                'Medium': sum(1 for r in risk_predictions if r == 'Medium'),\n",
        "                'Low': sum(1 for r in risk_predictions if r == 'Low')\n",
        "            }\n",
        "\n",
        "            # Get high-risk clauses\n",
        "            high_risk_clauses = []\n",
        "            for i, (clause, risk, probs) in enumerate(zip(valid_clauses, risk_predictions, risk_probabilities)):\n",
        "                if risk == 'High':\n",
        "                    high_risk_clauses.append({\n",
        "                        'clause': clause[:200] + \"...\" if len(clause) > 200 else clause,\n",
        "                        'risk_level': risk,\n",
        "                        'confidence': max(probs)\n",
        "                    })\n",
        "\n",
        "            return {\n",
        "                'summary': risk_summary,\n",
        "                'high_risk_clauses': high_risk_clauses[:10],  # Top 10 high-risk clauses\n",
        "                'total_analyzed': len(valid_clauses)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Risk analysis error: {e}\")\n",
        "            return {'error': f\"Risk analysis failed: {str(e)}\"}\n",
        "\n",
        "    def extract_key_terms(self, content: str) -> List[str]:\n",
        "        \"\"\"Extract key legal terms from content\"\"\"\n",
        "        legal_keywords = [\n",
        "            'agreement', 'contract', 'party', 'clause', 'termination', 'breach',\n",
        "            'liability', 'damages', 'indemnify', 'warranty', 'confidential',\n",
        "            'intellectual property', 'jurisdiction', 'governing law', 'force majeure',\n",
        "            'arbitration', 'penalty', 'liquidated damages', 'material adverse',\n",
        "            'license', 'copyright', 'trademark', 'patent', 'compliance',\n",
        "            'audit', 'report', 'renewal', 'expiration', 'fee', 'payment'\n",
        "        ]\n",
        "\n",
        "        content_lower = content.lower()\n",
        "        found_terms = [term for term in legal_keywords if term in content_lower]\n",
        "        return found_terms\n",
        "\n",
        "    def analyze_structure(self, content: str) -> Dict[str, Any]:\n",
        "        \"\"\"Analyze document structure\"\"\"\n",
        "        lines = content.split('\\\\n')\n",
        "\n",
        "        # Count different types of content\n",
        "        numbered_sections = len([line for line in lines if re.match(r'^\\\\d+\\\\.', line.strip())])\n",
        "        bullet_points = len([line for line in lines if re.match(r'^\\\\s*[â€¢\\\\-\\\\*]', line)])\n",
        "        all_caps_lines = len([line for line in lines if line.isupper() and len(line.strip()) > 5])\n",
        "\n",
        "        return {\n",
        "            'total_lines': len(lines),\n",
        "            'numbered_sections': numbered_sections,\n",
        "            'bullet_points': bullet_points,\n",
        "            'all_caps_lines': all_caps_lines,\n",
        "            'has_structure': numbered_sections > 0 or bullet_points > 0\n",
        "        }\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main Streamlit application\"\"\"\n",
        "\n",
        "    # Header\n",
        "    st.title(\"ğŸ“„ CUAD Legal Document QA System\")\n",
        "    st.markdown(\"*AI-powered legal document analysis and risk assessment*\")\n",
        "\n",
        "    # Initialize processor\n",
        "    if 'processor' not in st.session_state:\n",
        "        with st.spinner(\"ğŸ”§ Initializing legal document processor...\"):\n",
        "            st.session_state.processor = LegalDocumentProcessor()\n",
        "\n",
        "    # File upload section\n",
        "    st.header(\"ğŸ“¤ Upload Document\")\n",
        "    uploaded_file = st.file_uploader(\n",
        "        \"Choose a legal document\",\n",
        "        type=['pdf', 'txt', 'json', 'xlsx', 'csv'],\n",
        "        help=\"Upload PDF, TXT, JSON, XLSX, or CSV files for analysis\"\n",
        "    )\n",
        "\n",
        "    if uploaded_file is not None:\n",
        "        # Display file info\n",
        "        st.success(f\"âœ… File '{uploaded_file.name}' uploaded successfully!\")\n",
        "\n",
        "        # File details\n",
        "        file_size = len(uploaded_file.getvalue())\n",
        "        st.info(f\"ğŸ“Š File size: {file_size / 1024:.1f} KB\")\n",
        "\n",
        "        # Process button\n",
        "        if st.button(\"ğŸ” Analyze Document\", type=\"primary\"):\n",
        "            with st.spinner(\"ğŸ”„ Processing document...\"):\n",
        "                results = st.session_state.processor.process_file(uploaded_file)\n",
        "\n",
        "            # Display results\n",
        "            if results['success']:\n",
        "                display_analysis_results(results)\n",
        "            else:\n",
        "                st.error(f\"âŒ {results['error']}\")\n",
        "                if 'supported_formats' in results:\n",
        "                    st.info(f\"ğŸ’¡ Supported formats: {', '.join(results['supported_formats'])}\")\n",
        "\n",
        "def display_analysis_results(results: Dict[str, Any]):\n",
        "    \"\"\"Display comprehensive analysis results\"\"\"\n",
        "\n",
        "    st.success(\"âœ… Document processed successfully!\")\n",
        "\n",
        "    # Basic information\n",
        "    col1, col2, col3 = st.columns(3)\n",
        "    with col1:\n",
        "        st.metric(\"File Type\", results['file_type'].upper())\n",
        "    with col2:\n",
        "        st.metric(\"Processed At\", results['processed_at'].split()[1])\n",
        "    with col3:\n",
        "        st.metric(\"Status\", \"âœ… Complete\")\n",
        "\n",
        "    # Content preview\n",
        "    with st.expander(\"ğŸ“– Document Preview\", expanded=False):\n",
        "        st.text(results['content_preview'])\n",
        "\n",
        "    # Analysis results\n",
        "    analysis = results.get('analysis', {})\n",
        "\n",
        "    if 'basic_stats' in analysis:\n",
        "        st.subheader(\"ğŸ“Š Document Statistics\")\n",
        "        stats = analysis['basic_stats']\n",
        "\n",
        "        col1, col2, col3 = st.columns(3)\n",
        "        with col1:\n",
        "            st.metric(\"Word Count\", f\"{stats['word_count']:,}\")\n",
        "        with col2:\n",
        "            st.metric(\"Characters\", f\"{stats['character_count']:,}\")\n",
        "        with col3:\n",
        "            st.metric(\"Clauses Found\", stats['clause_count'])\n",
        "\n",
        "    # Risk analysis\n",
        "    if 'risk_analysis' in analysis and analysis['risk_analysis']:\n",
        "        risk_data = analysis['risk_analysis']\n",
        "\n",
        "        if 'error' not in risk_data:\n",
        "            st.subheader(\"âš ï¸ Risk Assessment\")\n",
        "\n",
        "            # Risk summary\n",
        "            if 'summary' in risk_data:\n",
        "                summary = risk_data['summary']\n",
        "                col1, col2, col3 = st.columns(3)\n",
        "                with col1:\n",
        "                    st.metric(\"ğŸ”´ High Risk\", summary.get('High', 0))\n",
        "                with col2:\n",
        "                    st.metric(\"ğŸŸ¡ Medium Risk\", summary.get('Medium', 0))\n",
        "                with col3:\n",
        "                    st.metric(\"ğŸŸ¢ Low Risk\", summary.get('Low', 0))\n",
        "\n",
        "            # High-risk clauses\n",
        "            if 'high_risk_clauses' in risk_data and risk_data['high_risk_clauses']:\n",
        "                st.subheader(\"ğŸš¨ High-Risk Clauses\")\n",
        "                for i, clause_data in enumerate(risk_data['high_risk_clauses']):\n",
        "                    with st.expander(f\"Risk Item #{i+1} (Confidence: {clause_data['confidence']:.2f})\"):\n",
        "                        st.write(clause_data['clause'])\n",
        "            else:\n",
        "                st.success(\"âœ… No high-risk clauses detected!\")\n",
        "        else:\n",
        "            st.warning(f\"âš ï¸ Risk analysis issue: {risk_data['error']}\")\n",
        "\n",
        "    # Key terms\n",
        "    if 'key_terms' in analysis and analysis['key_terms']:\n",
        "        st.subheader(\"ğŸ”‘ Key Legal Terms Found\")\n",
        "        terms_text = \", \".join(analysis['key_terms'])\n",
        "        st.write(terms_text)\n",
        "\n",
        "    # Structure analysis\n",
        "    if 'structure' in analysis:\n",
        "        structure = analysis['structure']\n",
        "        st.subheader(\"ğŸ“‹ Document Structure\")\n",
        "\n",
        "        col1, col2 = st.columns(2)\n",
        "        with col1:\n",
        "            st.metric(\"Total Lines\", structure['total_lines'])\n",
        "            st.metric(\"Numbered Sections\", structure['numbered_sections'])\n",
        "        with col2:\n",
        "            st.metric(\"Bullet Points\", structure['bullet_points'])\n",
        "            structured = \"âœ… Well Structured\" if structure['has_structure'] else \"âš ï¸ Unstructured\"\n",
        "            st.metric(\"Structure Quality\", structured)\n",
        "\n",
        "    # Sample clauses\n",
        "    if 'clauses_sample' in analysis and analysis['clauses_sample']:\n",
        "        with st.expander(\"ğŸ“ Sample Clauses\", expanded=False):\n",
        "            for i, clause in enumerate(analysis['clauses_sample'], 1):\n",
        "                st.write(f\"**{i}.** {clause}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "'''\n",
        "\n",
        "# Write the app to file\n",
        "with open('legal_qa_app.py', 'w') as f:\n",
        "    f.write(app_code)\n",
        "\n",
        "print(\"âœ… legal_qa_app.py created successfully!\")\n",
        "\n",
        "# STEP 4: Setup ngrok (for Colab)\n",
        "print(\"\\nğŸŒ Setting up ngrok for public access...\")\n",
        "\n",
        "# Mount Google Drive (if in Colab)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"âœ… Google Drive mounted\")\n",
        "except:\n",
        "    print(\"â„¹ï¸ Not in Colab environment - skipping Drive mount\")\n",
        "\n",
        "print(\"\\nğŸš€ Setup complete! Now run:\")\n",
        "print(\"1. !streamlit run legal_qa_app.py &>/content/app.log &\")\n",
        "print(\"2. Then setup ngrok tunnel\")\n",
        "print(\"3. Upload your documents and get real analysis results!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzd4eHtchW-G",
        "outputId": "61bfd914-5851-4fe8-e04e-2c6e5007c9ca"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ Installing required packages...\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.47.1-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.12-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.5.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.12.0.88)\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.3.0)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Collecting pdfminer.six==20250506 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.48.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.7.14)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.26.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
            "Downloading streamlit-1.47.1-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyngrok-7.2.12-py3-none-any.whl (26 kB)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m103.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, python-docx, pytesseract, pypdfium2, pyngrok, pydeck, pdfminer.six, pdfplumber, streamlit\n",
            "Successfully installed pdfminer.six-20250506 pdfplumber-0.11.7 pydeck-0.9.1 pyngrok-7.2.12 pypdfium2-4.30.0 pytesseract-0.3.13 python-docx-1.2.0 streamlit-1.47.1 watchdog-6.0.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "âœ… All packages installed successfully!\n",
            "\n",
            "ğŸ§ª Testing imports...\n",
            "âœ… All imports successful!\n",
            "\n",
            "ğŸ“ Creating legal_qa_app.py...\n",
            "âœ… legal_qa_app.py created successfully!\n",
            "\n",
            "ğŸŒ Setting up ngrok for public access...\n",
            "Mounted at /content/drive\n",
            "âœ… Google Drive mounted\n",
            "\n",
            "ğŸš€ Setup complete! Now run:\n",
            "1. !streamlit run legal_qa_app.py &>/content/app.log &\n",
            "2. Then setup ngrok tunnel\n",
            "3. Upload your documents and get real analysis results!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run legal_qa_app.py &>/content/app.log &"
      ],
      "metadata": {
        "id": "bT3gAxGehqt3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Kill any existing tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# Set your ngrok auth token\n",
        "os.system(\"ngrok config add-authtoken 30doYIfaLFxn5sAhPxx2v65UjRO_4y3pgAdUt4kUSmdYkVR4v\")\n",
        "\n",
        "# Wait a moment (not always necessary, but safe)\n",
        "time.sleep(2)\n",
        "\n",
        "# Start a new tunnel on port 8501 (default Streamlit port)\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"ğŸ”— Your Streamlit app is live at:\", public_url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XILEh0iWhwIn",
        "outputId": "d6966dfb-562d-4cd5-963a-c94dc3cbab4e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”— Your Streamlit app is live at: NgrokTunnel: \"https://be1fa69211a6.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ]
}